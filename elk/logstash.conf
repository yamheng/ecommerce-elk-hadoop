input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics           => ["orders"]
    group_id         => "logstash-orders-hdfs-v3"   # 用新的 group，避免用旧 offset
    auto_offset_reset => "earliest"
    codec            => json
  }
}

filter {
  date {
    match          => ["order_time", "ISO8601"]
    target         => "@timestamp"
    tag_on_failure => []
  }

  mutate {
    add_field => {
      "csv_line" => "%{order_id},%{user_id},%{product_id},%{category},%{price},%{quantity},%{order_time},%{status}"
    }
  }
}

output {
  # ====== 写 Elasticsearch（实时） ======
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "orders-%{+YYYY.MM.dd}"
  }

  # ====== 写 HDFS（真正让 Hive 能读的部分）======
  webhdfs {
    host => "namenode"
    port => 9870                 # ← Hadoop 3.x WebHDFS
    user => "root"

    # 每条订单写一个文件，绝不会冲突
    path => "/data/orders/order-%{+YYYY-MM-dd-HH-mm-ss-SSS}-%{order_id}.log"

    flush_size => 1
    idle_flush_time => 1

    codec => line {
      format => "%{csv_line}"
    }
  }

  stdout {
    codec => rubydebug
  }
}
